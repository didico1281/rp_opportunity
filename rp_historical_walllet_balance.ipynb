{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c732783-f606-4feb-82d8-c6b5d4a53db1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.mkdirs(\"/Volumes/workspace/recargapay/vol_rp/bronze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13161427-f782-4a7c-a7db-904d1164c1ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.mkdirs(\"/Volumes/workspace/recargapay/vol_rp/silver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "297c5837-4725-43c1-b52b-dde16db4d7bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.mkdirs(\"/Volumes/workspace/recargapay/vol_rp/gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adc78886-02d2-4bc1-8998-d37563f75101",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"event_time\":217},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754242605884}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('parquet').option('header', 'true').option('inferSchema', 'true').load(\"/Volumes/workspace/recargapay/vol_rp/bronze/*.parquet\").orderBy(\"account_id\", \"event_time\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84695b3c-583c-4436-8675-72468e98bb98",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"account_id\":303},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754244126168}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, to_date, to_timestamp, explode, sequence, last, row_number, min as _min, max as _max\n",
    ")\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# STEP 1: Parse event_time and extract date\n",
    "df = df.withColumn(\"event_time\", to_timestamp(\"event_time\")) \\\n",
    "       .withColumn(\"event_date\", to_date(\"event_time\"))\n",
    "\n",
    "# STEP 2: Get the last snapshot of balance for each day per account_id\n",
    "window_daily = Window.partitionBy(\"account_id\", \"event_date\").orderBy(col(\"event_time\").desc())\n",
    "\n",
    "daily_balances = df.withColumn(\"rn\", row_number().over(window_daily)) \\\n",
    "                   .filter(col(\"rn\") == 1) \\\n",
    "                   .select(\"account_id\", col(\"event_date\").alias(\"date\"), \"amount\")\n",
    "\n",
    "# STEP 3: Create a date range from the full dataset range\n",
    "min_date = df.agg(_min(\"event_date\")).first()[0]\n",
    "max_date = df.agg(_max(\"event_date\")).first()[0]\n",
    "\n",
    "calendar = spark.createDataFrame([(min_date, max_date)], [\"start\", \"end\"]) \\\n",
    "                .withColumn(\"date\", explode(sequence(col(\"start\"), col(\"end\")))) \\\n",
    "                .select(\"date\")\n",
    "\n",
    "# STEP 4: Cross join all dates with all account_ids\n",
    "account_ids = df.select(\"account_id\").distinct()\n",
    "full_dates = account_ids.crossJoin(calendar)\n",
    "\n",
    "# STEP 5: Join full date range with daily balances\n",
    "joined = full_dates.join(daily_balances, on=[\"account_id\", \"date\"], how=\"left\")\n",
    "\n",
    "# STEP 6: Forward-fill the last known balance for each account\n",
    "window_ffill = Window.partitionBy(\"account_id\").orderBy(\"date\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "final_df = joined.withColumn(\"balance\", last(\"amount\", ignorenulls=True).over(window_ffill)) \\\n",
    "                 .select(\"account_id\", \"date\", \"balance\") \\\n",
    "                 .orderBy(\"account_id\", \"date\")\n",
    "display(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a838ca8-6f89-4483-a4e8-fe9247979579",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, sum as spark_sum\n",
    "\n",
    "# Aggregate daily totals\n",
    "result = (\n",
    "    df.withColumn(\"date\", date_format(\"event_time\", \"yyyy-MM-dd\"))\n",
    "      .groupBy(\"account_id\", \"user_id\", \"date\")\n",
    "      .agg(spark_sum(\"amount\").alias(\"total_amount\"))\n",
    "      .orderBy(\"account_id\", \"date\")\n",
    ")\n",
    "\n",
    "# Write to Delta format\n",
    "result.write.mode(\"overwrite\").format(\"delta\").save(\"/Volumes/workspace/recargapay/vol_rp/silver/tb_daily_balance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3bf0965-bca6-4d26-ab7b-fc82d1eeb719",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, date_format, sum as _sum, explode, sequence, lit, min as _min, max as _max\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# STEP 1: Format date and aggregate total amount per day\n",
    "daily_totals = (\n",
    "    df.withColumn(\"date\", date_format(\"event_time\", \"yyyy-MM-dd\"))\n",
    "      .groupBy(\"account_id\", \"user_id\", \"date\")\n",
    "      .agg(_sum(\"amount\").alias(\"daily_amount\"))\n",
    ")\n",
    "\n",
    "# STEP 2: Generate full calendar date range\n",
    "min_date = df.select(date_format(\"event_time\", \"yyyy-MM-dd\").alias(\"date\")).agg(_min(\"date\")).first()[0]\n",
    "max_date = df.select(date_format(\"event_time\", \"yyyy-MM-dd\").alias(\"date\")).agg(_max(\"date\")).first()[0]\n",
    "\n",
    "calendar = spark.createDataFrame([(min_date, max_date)], [\"start\", \"end\"]) \\\n",
    "    .withColumn(\"date\", explode(sequence(lit(min_date).cast(\"date\"), lit(max_date).cast(\"date\"), lit(1).cast(\"interval day\")))) \\\n",
    "    .selectExpr(\"date_format(date, 'yyyy-MM-dd') as date\")\n",
    "\n",
    "# STEP 3: Cross join all account_id + user_id pairs with the full date range\n",
    "account_users = df.select(\"account_id\", \"user_id\").distinct()\n",
    "full_grid = account_users.crossJoin(calendar)\n",
    "\n",
    "# STEP 4: Join with actual daily totals and fill missing values\n",
    "daily_complete = full_grid.join(daily_totals, on=[\"account_id\", \"user_id\", \"date\"], how=\"left\") \\\n",
    "                          .fillna({\"daily_amount\": 0.0})\n",
    "\n",
    "# STEP 5: Define window and compute cumulative balance\n",
    "window_spec = Window.partitionBy(\"account_id\").orderBy(\"date\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "result = (\n",
    "    daily_complete.withColumn(\"daily_balance\", _sum(\"daily_amount\").over(window_spec))\n",
    "                  .orderBy(\"account_id\", \"date\")\n",
    ")\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "286a671b-ef97-46b2-a6e2-8bd9def35cb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result.write.mode(\"overwrite\").format(\"delta\").save(\"/Volumes/workspace/recargapay/vol_rp/silver/daily_balance_table\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "rp_historical_walllet_balance",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
